{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "962676b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.12.6)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "\n",
    "class TankEnv:\n",
    "    def __init__(self):\n",
    "        pygame.init()\n",
    "        self.width, self.height = 800, 600\n",
    "        self.screen = pygame.display.set_mode((self.width, self.height))\n",
    "        pygame.display.set_caption(\"Tank Reinforcement Learning Environment\")\n",
    "        self.clock = pygame.time.Clock()\n",
    "        self.font = pygame.font.SysFont('Arial', 16)\n",
    "\n",
    "        self.tank_speed = 3\n",
    "        self.tank_size = 20\n",
    "        self.target_radius = 15\n",
    "\n",
    "        # Препятствия\n",
    "        self.obstacles = [\n",
    "            pygame.Rect(300, 200, 200, 50),\n",
    "            pygame.Rect(400, 400, 100, 200),\n",
    "            pygame.Rect(100, 300, 150, 30),\n",
    "            pygame.Rect(600, 100, 50, 300)\n",
    "        ]\n",
    "\n",
    "        # Инициализация танка и цели\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Сброс среды в начальное состояние\"\"\"\n",
    "        self.tank = {\n",
    "            'x': random.randint(50, self.width-50),\n",
    "            'y': random.randint(50, self.height-50),\n",
    "            'angle': random.randint(0, 359),\n",
    "            'health': 100\n",
    "        }\n",
    "\n",
    "        self.target = {\n",
    "            'x': random.randint(50, self.width-50),\n",
    "            'y': random.randint(50, self.height-50)\n",
    "        }\n",
    "\n",
    "        # Проверка, чтобы цель не появлялась в препятствиях\n",
    "        target_rect = pygame.Rect(\n",
    "            self.target['x'] - self.target_radius,\n",
    "            self.target['y'] - self.target_radius,\n",
    "            self.target_radius * 2,\n",
    "            self.target_radius * 2\n",
    "        )\n",
    "\n",
    "        for obs in self.obstacles:\n",
    "            if target_rect.colliderect(obs):\n",
    "                return self.reset()  # Рекурсивно пробуем снова\n",
    "\n",
    "        return self._get_state()\n",
    "\n",
    "    def _get_state(self):\n",
    "        \"\"\"Получение текущего состояния среды\"\"\"\n",
    "        dx = self.target['x'] - self.tank['x']\n",
    "        dy = self.target['y'] - self.tank['y']\n",
    "        distance = math.sqrt(dx**2 + dy**2)\n",
    "        angle_to_target = math.atan2(dy, dx) - math.radians(self.tank['angle'])\n",
    "\n",
    "        # Нормализация угла в диапазон [-π, π]\n",
    "        angle_to_target = (angle_to_target + math.pi) % (2 * math.pi) - math.pi\n",
    "\n",
    "        # Определение расстояний до препятствий в 4 направлениях\n",
    "        obstacle_distances = self._get_obstacle_distances()\n",
    "\n",
    "        return np.array([\n",
    "            self.tank['x'] / self.width,  # Нормированная x-координата танка\n",
    "            self.tank['y'] / self.height,  # Нормированная y-координата танка\n",
    "            self.tank['angle'] / 360,     # Нормированный угол\n",
    "            distance / math.sqrt(self.width**2 + self.height**2),  # Нормированное расстояние\n",
    "            angle_to_target / math.pi,     # Нормированный угол до цели\n",
    "            *obstacle_distances           # Расстояния до препятствий\n",
    "        ], dtype=np.float32)\n",
    "\n",
    "    def _get_obstacle_distances(self):\n",
    "        \"\"\"Вычисление расстояний до препятствий в 4 направлениях\"\"\"\n",
    "        angles = [0, 90, 180, 270]  # Вперед, вправо, назад, влево\n",
    "        distances = []\n",
    "\n",
    "        for angle in angles:\n",
    "            ray_angle = math.radians(self.tank['angle'] + angle)\n",
    "            dist = self._cast_ray(ray_angle)\n",
    "            distances.append(dist / max(self.width, self.height))  # Нормализация\n",
    "\n",
    "        return distances\n",
    "\n",
    "    def _cast_ray(self, angle):\n",
    "        \"\"\"Бросок луча для определения расстояния до препятствия\"\"\"\n",
    "        step = 5\n",
    "        x, y = self.tank['x'], self.tank['y']\n",
    "\n",
    "        for d in range(0, 500, step):\n",
    "            x += step * math.cos(angle)\n",
    "            y += step * math.sin(angle)\n",
    "\n",
    "            # Проверка выхода за границы\n",
    "            if not (0 <= x <= self.width and 0 <= y <= self.height):\n",
    "                return d\n",
    "\n",
    "            # Проверка столкновения с препятствиями\n",
    "            for obs in self.obstacles:\n",
    "                if obs.collidepoint(x, y):\n",
    "                    return d\n",
    "\n",
    "        return 500  # Максимальное расстояние\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Выполнение действия и возврат нового состояния\"\"\"\n",
    "        reward = -0.1  # Штраф за каждый шаг\n",
    "        done = False\n",
    "        info = {'reached_target': False, 'hit_obstacle': False}\n",
    "\n",
    "        # Обработка действий\n",
    "        if action == 0:  # Вперед\n",
    "            self.tank['x'] += self.tank_speed * math.cos(math.radians(self.tank['angle']))\n",
    "            self.tank['y'] += self.tank_speed * math.sin(math.radians(self.tank['angle']))\n",
    "        elif action == 1:  # Назад\n",
    "            self.tank['x'] -= self.tank_speed * math.cos(math.radians(self.tank['angle']))\n",
    "            self.tank['y'] -= self.tank_speed * math.sin(math.radians(self.tank['angle']))\n",
    "        elif action == 2:  # Влево\n",
    "            self.tank['angle'] = (self.tank['angle'] - 5) % 360\n",
    "        elif action == 3:  # Вправо\n",
    "            self.tank['angle'] = (self.tank['angle'] + 5) % 360\n",
    "\n",
    "        # Проверка границ\n",
    "        self.tank['x'] = np.clip(self.tank['x'], 0, self.width)\n",
    "        self.tank['y'] = np.clip(self.tank['y'], 0, self.height)\n",
    "\n",
    "        # Проверка столкновений с препятствиями\n",
    "        tank_rect = pygame.Rect(\n",
    "            self.tank['x'] - self.tank_size//2,\n",
    "            self.tank['y'] - self.tank_size//2,\n",
    "            self.tank_size,\n",
    "            self.tank_size\n",
    "        )\n",
    "\n",
    "        for obs in self.obstacles:\n",
    "            if tank_rect.colliderect(obs):\n",
    "                reward = -10\n",
    "                done = True\n",
    "                info['hit_obstacle'] = True\n",
    "                break\n",
    "\n",
    "        # Проверка достижения цели\n",
    "        if math.dist((self.tank['x'], self.tank['y']),\n",
    "                    (self.target['x'], self.target['y'])) < self.target_radius + self.tank_size//2:\n",
    "            reward = 100\n",
    "            done = True\n",
    "            info['reached_target'] = True\n",
    "\n",
    "        # Дополнительные награды/штрафы\n",
    "        dx = self.target['x'] - self.tank['x']\n",
    "        dy = self.target['y'] - self.tank['y']\n",
    "        new_dist = math.sqrt(dx**2 + dy**2)\n",
    "\n",
    "        if hasattr(self, 'prev_dist'):\n",
    "            if new_dist < self.prev_dist:\n",
    "                reward += 0.5  # Награда за приближение\n",
    "            else:\n",
    "                reward -= 0.3  # Штраф за удаление\n",
    "        self.prev_dist = new_dist\n",
    "\n",
    "        return self._get_state(), reward, done, info\n",
    "\n",
    "    def render(self, episode=None, reward=None):\n",
    "        \"\"\"Отрисовка текущего состояния среды (исправленная версия)\"\"\"\n",
    "        self.screen.fill((240, 240, 240))\n",
    "\n",
    "        # Отрисовка препятствий\n",
    "        for obs in self.obstacles:\n",
    "            pygame.draw.rect(self.screen, (70, 70, 70), obs)\n",
    "\n",
    "        # Отрисовка цели (исправленный вызов pygame.draw.circle)\n",
    "        pygame.draw.circle(\n",
    "            self.screen,\n",
    "            (255, 50, 50),\n",
    "            (int(self.target['x']), int(self.target['y'])),  # Центр как tuple (x,y)\n",
    "            self.target_radius  # Радиус\n",
    "        )\n",
    "\n",
    "        # Отрисовка танка\n",
    "        tank_color = (50, 150, 255) if self.tank['health'] > 50 else (255, 150, 50)\n",
    "        tank_points = [\n",
    "            (self.tank['x'] + self.tank_size*math.cos(math.radians(self.tank['angle']))),\n",
    "            (self.tank['y'] + self.tank_size*math.sin(math.radians(self.tank['angle']))),\n",
    "            (self.tank['x'] + (self.tank_size//2)*math.cos(math.radians(self.tank['angle']+120))),\n",
    "            (self.tank['y'] + (self.tank_size//2)*math.sin(math.radians(self.tank['angle']+120))),\n",
    "            (self.tank['x'] + (self.tank_size//2)*math.cos(math.radians(self.tank['angle']-120))),\n",
    "            (self.tank['y'] + (self.tank_size//2)*math.sin(math.radians(self.tank['angle']-120)))\n",
    "        ]\n",
    "        pygame.draw.polygon(self.screen, tank_color, tank_points)\n",
    "\n",
    "        # Отрисовка информации\n",
    "        if episode is not None and reward is not None:\n",
    "            info_text = f\"Episode: {episode} | Reward: {reward:.1f} | Angle: {self.tank['angle']}°\"\n",
    "            text_surface = self.font.render(info_text, True, (0, 0, 0))\n",
    "            self.screen.blit(text_surface, (10, 10))\n",
    "\n",
    "        pygame.display.flip()\n",
    "        self.clock.tick(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "496f2ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# State (пример для DQN)\n",
    "state_size = 5  # [норм_x, норм_y, норм_угол, норм_расст, норм_угол_к_цели]\n",
    "\n",
    "# Actions\n",
    "action_space = [\n",
    "    \"вперед\", \"назад\", \"влево\", \"вправо\", \"огонь\"\n",
    "]\n",
    "\n",
    "# Reward function (дополнение к коду среды)\n",
    "def calculate_reward(self):\n",
    "    reward = -1  # Штраф за шаг\n",
    "    prev_dist = math.dist(self.prev_pos, (self.target['x'], self.target['y']))\n",
    "    curr_dist = math.dist((self.tank['x'], self.tank['y']),\n",
    "                         (self.target['x'], self.target['y']))\n",
    "\n",
    "    if curr_dist < prev_dist:\n",
    "        reward += 10  # Награда за приближение\n",
    "    else:\n",
    "        reward -= 5   # Штраф за удаление\n",
    "\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d071fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arau7\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Эпизод:  1/10, Награда: -7081.60, Epsilon: 1.000, Loss: 0.0000\n",
      "Эпизод:  2/10, Награда:  -10.30, Epsilon: 0.998, Loss: 0.0139\n",
      "Эпизод:  3/10, Награда: -3449.50, Epsilon: 0.996, Loss: 0.0135\n",
      "Эпизод:  4/10, Награда:   -9.50, Epsilon: 0.994, Loss: 0.0132\n",
      "Эпизод:  5/10, Награда: -5434.30, Epsilon: 0.992, Loss: 0.0133\n",
      "Эпизод:  6/10, Награда:   -9.50, Epsilon: 0.990, Loss: 0.0133\n",
      "Эпизод:  7/10, Награда:  -10.30, Epsilon: 0.988, Loss: 0.0133\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=100000)\n",
    "        self.gamma = 0.99\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.998\n",
    "        self.batch_size = 128\n",
    "        self.model = self._build_model()\n",
    "        self.target_model = self._build_model()\n",
    "        self.update_target_model()\n",
    "        self.loss_history = []\n",
    "\n",
    "    def _build_model(self):\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(64, input_dim=self.state_size, activation='relu'),\n",
    "            tf.keras.layers.Dense(64, activation='relu'),\n",
    "            tf.keras.layers.Dense(self.action_size, activation='linear')\n",
    "        ])\n",
    "        model.compile(\n",
    "            loss=tf.keras.losses.Huber(),\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=0.00025),\n",
    "            metrics=['mae']\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        state = np.reshape(state, [1, self.state_size])\n",
    "        return np.argmax(self.model.predict(state, verbose=0)[0])\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        minibatch = random.sample(self.memory, self.batch_size)\n",
    "        states = np.array([x[0] for x in minibatch])\n",
    "        actions = np.array([x[1] for x in minibatch])\n",
    "        rewards = np.array([x[2] for x in minibatch])\n",
    "        next_states = np.array([x[3] for x in minibatch])\n",
    "        dones = np.array([x[4] for x in minibatch])\n",
    "\n",
    "        # Double DQN\n",
    "        current_q = self.model.predict(states, verbose=0)\n",
    "        next_q = self.model.predict(next_states, verbose=0)\n",
    "        next_target_q = self.target_model.predict(next_states, verbose=0)\n",
    "\n",
    "        max_actions = np.argmax(next_q, axis=1)\n",
    "        targets = rewards + self.gamma * next_target_q[np.arange(self.batch_size), max_actions] * (1 - dones)\n",
    "\n",
    "        current_q[np.arange(self.batch_size), actions] = targets\n",
    "\n",
    "        history = self.model.fit(\n",
    "            states,\n",
    "            current_q,\n",
    "            batch_size=self.batch_size,\n",
    "            verbose=0\n",
    "        )\n",
    "        self.loss_history.append(history.history['loss'][0])\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "# Инициализация и обучение (20 эпизодов)\n",
    "env = TankEnv()\n",
    "agent = DQNAgent(state_size=9, action_size=5)\n",
    "episodes = 10\n",
    "target_update_freq = 2  # Обновлять target network каждые 10 эпизодов\n",
    "reward_history = []\n",
    "\n",
    "for e in range(episodes):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    while True:\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "        if done:\n",
    "            reward_history.append(total_reward)\n",
    "            if e % target_update_freq == 0:\n",
    "                agent.update_target_model()\n",
    "\n",
    "            print(f\"Эпизод: {e+1:2d}/{episodes}, \"\n",
    "                  f\"Награда: {total_reward:7.2f}, \"\n",
    "                  f\"Epsilon: {agent.epsilon:.3f}, \"\n",
    "                  f\"Loss: {np.mean(agent.loss_history[-10:] if agent.loss_history else 0):.4f}\")\n",
    "            break\n",
    "\n",
    "    agent.replay()\n",
    "\n",
    "# Вывод итоговой статистики\n",
    "print(\"\\nОбучение завершено!\")\n",
    "print(f\"Средняя награда: {np.mean(reward_history):.2f}\")\n",
    "print(f\"Максимальная награда: {max(reward_history):.2f}\")\n",
    "print(f\"Минимальная награда: {min(reward_history):.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
