{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c12a4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962676b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "TRAINING_MODE = True\n",
    "RENDER_SPEED = 50.0\n",
    "\n",
    "class TankEnv:\n",
    "    def __init__(self):\n",
    "        pygame.init()\n",
    "        self.width, self.height = 1000, 800\n",
    "        self.screen = pygame.display.set_mode((self.width, self.height))\n",
    "        pygame.display.set_caption(\"Tank RL Environment v2.3\")\n",
    "        self.clock = pygame.time.Clock()\n",
    "        self.font = pygame.font.SysFont('Arial', 20)\n",
    "        \n",
    "        # Параметры танка\n",
    "        self.tank_speed = 4\n",
    "        self.tank_size = 25\n",
    "        self.target_radius = 20\n",
    "        self.bullet_speed = 10\n",
    "        self.bullet_radius = 4\n",
    "        self.bullets = []\n",
    "        \n",
    "        # Улучшенная система препятствий с граничными стенами\n",
    "        wall_thickness = 20\n",
    "        self.obstacles = [\n",
    "            pygame.Rect(200, 150, 300, 40),\n",
    "            pygame.Rect(500, 300, 40, 200),\n",
    "            pygame.Rect(100, 500, 250, 30),\n",
    "            pygame.Rect(700, 200, 50, 400),\n",
    "            pygame.Rect(300, 400, 200, 50),\n",
    "            pygame.Rect(600, 100, 150, 40),\n",
    "            pygame.Rect(0, 0, self.width, wall_thickness),\n",
    "            pygame.Rect(0, self.height - wall_thickness, self.width, wall_thickness),\n",
    "            pygame.Rect(0, 0, wall_thickness, self.height),\n",
    "            pygame.Rect(self.width - wall_thickness, 0, wall_thickness, self.height)\n",
    "        ]\n",
    "        \n",
    "        # Инициализация\n",
    "        self.reset()\n",
    "        self.action_descriptions = [\"ВПЕРЕД\", \"НАЗАД\", \"ВЛЕВО\", \"ВПРАВО\", \"ОГОНЬ\"]\n",
    "        \n",
    "        # Стартовый рендеринг для инициализации экрана\n",
    "        self.screen.fill((230, 230, 230))\n",
    "        pygame.display.flip()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Сброс среды в начальное состояние\"\"\"\n",
    "        # Генерация позиций танка и цели без пересечения с препятствиями\n",
    "        max_attempts = 100\n",
    "        for attempt in range(max_attempts):\n",
    "            self.tank = {\n",
    "                'x': random.randint(50, self.width-50),\n",
    "                'y': random.randint(50, self.height-50),\n",
    "                'angle': random.randint(0, 359),\n",
    "                'health': 100,\n",
    "                'cooldown': 0\n",
    "            }\n",
    "            \n",
    "            self.target = {\n",
    "                'x': random.randint(50, self.width-50),\n",
    "                'y': random.randint(50, self.height-50)\n",
    "            }\n",
    "            \n",
    "            # Проверка коллизий\n",
    "            tank_rect = pygame.Rect(\n",
    "                self.tank['x'] - self.tank_size//2,\n",
    "                self.tank['y'] - self.tank_size//2,\n",
    "                self.tank_size, self.tank_size\n",
    "            )\n",
    "            \n",
    "            target_rect = pygame.Rect(\n",
    "                self.target['x'] - self.target_radius,\n",
    "                self.target['y'] - self.target_radius,\n",
    "                self.target_radius*2, self.target_radius*2\n",
    "            )\n",
    "            \n",
    "            valid_positions = True\n",
    "            for obs in self.obstacles:\n",
    "                if tank_rect.colliderect(obs) or target_rect.colliderect(obs):\n",
    "                    valid_positions = False\n",
    "                    break\n",
    "            \n",
    "            if valid_positions:\n",
    "                break\n",
    "            elif attempt == max_attempts - 1:\n",
    "                print(\"Предупреждение: не удалось найти валидные позиции после 100 попыток\")\n",
    "        \n",
    "        self.bullets = []\n",
    "        self.prev_dist = math.dist((self.tank['x'], self.tank['y']), \n",
    "                                  (self.target['x'], self.target['y']))\n",
    "        return self._get_state()\n",
    "\n",
    "    def _get_state(self):\n",
    "        \"\"\"Получение текущего состояния среды\"\"\"\n",
    "        dx = self.target['x'] - self.tank['x']\n",
    "        dy = self.target['y'] - self.tank['y']\n",
    "        distance = math.sqrt(dx**2 + dy**2)\n",
    "        angle_to_target = math.atan2(dy, dx) - math.radians(self.tank['angle'])\n",
    "        angle_to_target = (angle_to_target + math.pi) % (2 * math.pi) - math.pi\n",
    "        \n",
    "        # Расстояния до препятствий в 8 направлениях\n",
    "        obstacle_distances = self._get_obstacle_distances(num_rays=8)\n",
    "        \n",
    "        # Состояние здоровья\n",
    "        health_state = self.tank['health'] / 100.0\n",
    "        \n",
    "        return np.array([\n",
    "            self.tank['x'] / self.width,\n",
    "            self.tank['y'] / self.height,\n",
    "            self.tank['angle'] / 360.0,\n",
    "            distance / math.sqrt(self.width**2 + self.height**2),\n",
    "            angle_to_target / math.pi,\n",
    "            health_state,\n",
    "            *obstacle_distances\n",
    "        ], dtype=np.float32)\n",
    "\n",
    "    def _get_obstacle_distances(self, num_rays=16):\n",
    "        \"\"\"Вычисление расстояний до препятствий в различных направлениях\"\"\"\n",
    "        distances = []\n",
    "        for i in range(num_rays):\n",
    "            angle = math.radians(self.tank['angle'] + (360/num_rays)*i)\n",
    "            dist = self._cast_ray(angle)\n",
    "            distances.append(dist / max(self.width, self.height))\n",
    "        return distances\n",
    "\n",
    "    def _cast_ray(self, angle):\n",
    "        \"\"\"Трассировка луча для обнаружения препятствий\"\"\"\n",
    "        step = 3\n",
    "        x, y = self.tank['x'], self.tank['y']\n",
    "        \n",
    "        for d in range(0, 600, step):\n",
    "            x += step * math.cos(angle)\n",
    "            y += step * math.sin(angle)\n",
    "            \n",
    "            # Проверка границ\n",
    "            if not (0 <= x <= self.width and 0 <= y <= self.height):\n",
    "                return d\n",
    "                \n",
    "            # Проверка столкновения с препятствиями\n",
    "            for obs in self.obstacles:\n",
    "                if obs.collidepoint(x, y):\n",
    "                    return d\n",
    "                    \n",
    "        return 600\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Выполнение действия и возврат нового состояния\"\"\"\n",
    "        reward = -0.2\n",
    "        done = False\n",
    "        info = {'reached_target': False, 'hit_obstacle': False, 'target_hit': False}\n",
    "        \n",
    "        # Обработка кулдауна стрельбы\n",
    "        if self.tank['cooldown'] > 0:\n",
    "            self.tank['cooldown'] -= 1\n",
    "        \n",
    "        # Обработка действий\n",
    "        if action == 0:  # Вперед\n",
    "            self.tank['x'] += self.tank_speed * math.cos(math.radians(self.tank['angle']))\n",
    "            self.tank['y'] += self.tank_speed * math.sin(math.radians(self.tank['angle']))\n",
    "        elif action == 1:  # Назад\n",
    "            self.tank['x'] -= self.tank_speed * math.cos(math.radians(self.tank['angle']))\n",
    "            self.tank['y'] -= self.tank_speed * math.sin(math.radians(self.tank['angle']))\n",
    "        elif action == 2:  # Влево\n",
    "            self.tank['angle'] = (self.tank['angle'] - 5) % 360\n",
    "        elif action == 3:  # Вправо\n",
    "            self.tank['angle'] = (self.tank['angle'] + 5) % 360\n",
    "        elif action == 4 and self.tank['cooldown'] == 0:  # Огонь\n",
    "            self.bullets.append({\n",
    "                'x': self.tank['x'] + (self.tank_size+5) * math.cos(math.radians(self.tank['angle'])),\n",
    "                'y': self.tank['y'] + (self.tank_size+5) * math.sin(math.radians(self.tank['angle'])),\n",
    "                'dx': self.bullet_speed * math.cos(math.radians(self.tank['angle'])),\n",
    "                'dy': self.bullet_speed * math.sin(math.radians(self.tank['angle']))\n",
    "            })\n",
    "            self.tank['cooldown'] = 10\n",
    "        \n",
    "        # Обновление пуль\n",
    "        for bullet in self.bullets[:]:\n",
    "            bullet['x'] += bullet['dx']\n",
    "            bullet['y'] += bullet['dy']\n",
    "            \n",
    "            # Проверка попадания в цель\n",
    "            if math.dist((bullet['x'], bullet['y']), \n",
    "                        (self.target['x'], self.target['y'])) < self.target_radius + self.bullet_radius:\n",
    "                reward = 100\n",
    "                done = True\n",
    "                info['target_hit'] = True\n",
    "                self.bullets.remove(bullet)\n",
    "                break\n",
    "                \n",
    "            # Проверка выхода за границы\n",
    "            if not (0 <= bullet['x'] <= self.width and 0 <= bullet['y'] <= self.height):\n",
    "                self.bullets.remove(bullet)\n",
    "                continue\n",
    "                \n",
    "            # Проверка столкновения с препятствиями\n",
    "            for obs in self.obstacles:\n",
    "                if obs.collidepoint(bullet['x'], bullet['y']):\n",
    "                    if bullet in self.bullets:\n",
    "                        self.bullets.remove(bullet)\n",
    "                    break\n",
    "        \n",
    "        # Ограничение позиции танка\n",
    "        self.tank['x'] = np.clip(self.tank['x'], 0, self.width)\n",
    "        self.tank['y'] = np.clip(self.tank['y'], 0, self.height)\n",
    "        \n",
    "        # Проверка столкновения танка с препятствиями\n",
    "        tank_rect = pygame.Rect(\n",
    "            self.tank['x'] - self.tank_size//2,\n",
    "            self.tank['y'] - self.tank_size//2,\n",
    "            self.tank_size, self.tank_size\n",
    "        )\n",
    "        \n",
    "        for obs in self.obstacles:\n",
    "            if tank_rect.colliderect(obs):\n",
    "                reward = -15\n",
    "                self.tank['health'] -= 20\n",
    "                if self.tank['health'] <= 0:\n",
    "                    reward = -30\n",
    "                    done = True\n",
    "                    info['hit_obstacle'] = True\n",
    "                break\n",
    "        \n",
    "        # Проверка достижения цели\n",
    "        if math.dist((self.tank['x'], self.tank['y']),\n",
    "                    (self.target['x'], self.target['y'])) < self.target_radius + self.tank_size//2:\n",
    "            reward = 80\n",
    "            done = True\n",
    "            info['reached_target'] = True\n",
    "        \n",
    "        # Награда за приближение/удаление\n",
    "        new_dist = math.dist((self.tank['x'], self.tank['y']),\n",
    "                            (self.target['x'], self.target['y']))\n",
    "        \n",
    "        if new_dist < self.prev_dist:\n",
    "            reward += 1.0\n",
    "        else:\n",
    "            reward -= 0.8\n",
    "            \n",
    "        self.prev_dist = new_dist\n",
    "        \n",
    "        return self._get_state(), reward, done, info\n",
    "\n",
    "    def render(self, episode=None, total_reward=None, action=None, speed=1.0):\n",
    "        \"\"\"Отрисовка текущего состояния среды\"\"\"\n",
    "        # Обработка событий\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                pygame.quit()\n",
    "                return False\n",
    "        \n",
    "        # Отрисовка\n",
    "        self.screen.fill((230, 230, 230))\n",
    "        \n",
    "        # Отрисовка препятствий\n",
    "        for i, obs in enumerate(self.obstacles):\n",
    "            if i < 6:\n",
    "                pygame.draw.rect(self.screen, (90, 90, 120), obs)\n",
    "                pygame.draw.rect(self.screen, (50, 50, 80), obs, 2)\n",
    "            else:\n",
    "                pygame.draw.rect(self.screen, (150, 150, 160), obs)\n",
    "                pygame.draw.rect(self.screen, (100, 100, 120), obs, 2)\n",
    "        \n",
    "        # Отрисовка цели\n",
    "        pygame.draw.circle(self.screen, (220, 50, 50), \n",
    "                          (int(self.target['x']), int(self.target['y'])), \n",
    "                          self.target_radius)\n",
    "        pygame.draw.circle(self.screen, (180, 30, 30), \n",
    "                          (int(self.target['x']), int(self.target['y'])), \n",
    "                          self.target_radius, 2)\n",
    "        \n",
    "        # Отрисовка пуль\n",
    "        for bullet in self.bullets:\n",
    "            pygame.draw.circle(self.screen, (30, 30, 30), \n",
    "                             (int(bullet['x']), int(bullet['y'])), \n",
    "                             self.bullet_radius)\n",
    "        \n",
    "        # Отрисовка танка\n",
    "        angle_rad = math.radians(self.tank['angle'])\n",
    "        tank_color = (50, 180, 100) if self.tank['health'] > 50 else (220, 140, 60)\n",
    "        \n",
    "        points = [\n",
    "            (self.tank['x'] + self.tank_size * math.cos(angle_rad),\n",
    "             self.tank['y'] + self.tank_size * math.sin(angle_rad)),\n",
    "            (self.tank['x'] + self.tank_size//2 * math.cos(angle_rad + 2.4),\n",
    "             self.tank['y'] + self.tank_size//2 * math.sin(angle_rad + 2.4)),\n",
    "            (self.tank['x'] + self.tank_size//2 * math.cos(angle_rad - 2.4),\n",
    "             self.tank['y'] + self.tank_size//2 * math.sin(angle_rad - 2.4))\n",
    "        ]\n",
    "        \n",
    "        pygame.draw.polygon(self.screen, tank_color, points)\n",
    "        pygame.draw.polygon(self.screen, (30, 80, 50), points, 2)\n",
    "        \n",
    "        # Линия направления\n",
    "        end_x = self.tank['x'] + (self.tank_size+10) * math.cos(angle_rad)\n",
    "        end_y = self.tank['y'] + (self.tank_size+10) * math.sin(angle_rad)\n",
    "        pygame.draw.line(self.screen, (30, 30, 30), \n",
    "                        (self.tank['x'], self.tank['y']), \n",
    "                        (end_x, end_y), 2)\n",
    "        \n",
    "        # Панель информации\n",
    "        info_y = 10\n",
    "        if episode is not None:\n",
    "            text = self.font.render(f\"Эпизод: {episode}\", True, (0, 0, 0))\n",
    "            self.screen.blit(text, (10, info_y))\n",
    "            info_y += 25\n",
    "            \n",
    "        if total_reward is not None:\n",
    "            text = self.font.render(f\"Награда: {total_reward:.1f}\", True, (0, 0, 0))\n",
    "            self.screen.blit(text, (10, info_y))\n",
    "            info_y += 25\n",
    "            \n",
    "        if action is not None and 0 <= action < len(self.action_descriptions):\n",
    "            text = self.font.render(f\"Действие: {self.action_descriptions[action]}\", True, (0, 100, 0))\n",
    "            self.screen.blit(text, (10, info_y))\n",
    "            info_y += 25\n",
    "            \n",
    "        # Полоса здоровья\n",
    "        health_width = max(0, min(150, self.tank['health'] * 1.5))\n",
    "        pygame.draw.rect(self.screen, (200, 200, 200), (self.width-160, 15, 150, 20))\n",
    "        pygame.draw.rect(self.screen, (220, 80, 60) if self.tank['health'] < 50 else (80, 180, 100), \n",
    "                        (self.width-160, 15, health_width, 20))\n",
    "        pygame.draw.rect(self.screen, (100, 100, 100), (self.width-160, 15, 150, 20), 2)\n",
    "        health_text = self.font.render(f\"Здоровье: {self.tank['health']}%\", True, (0, 0, 0))\n",
    "        self.screen.blit(health_text, (self.width-155, 16))\n",
    "        \n",
    "        # Обновление экрана\n",
    "        pygame.display.flip()\n",
    "        \n",
    "        # Управление скоростью\n",
    "        if speed > 0:\n",
    "            self.clock.tick(60 * speed)\n",
    "            \n",
    "        return True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4bf7b44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=200000)\n",
    "        self.gamma = 0.95\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.0005\n",
    "        self.batch_size = 128\n",
    "        self.model = self._build_model()\n",
    "        self.target_model = self._build_model()\n",
    "        self.update_target_model()\n",
    "        self.loss_history = []\n",
    "\n",
    "    def _build_model(self):\n",
    "        model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(128, input_dim=self.state_size, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(self.action_size, activation='linear')\n",
    "        ])\n",
    "        \n",
    "        model.compile(\n",
    "            loss=tf.keras.losses.Huber(),\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=self.learning_rate),\n",
    "            metrics=['mae']\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "            \n",
    "        state = np.reshape(state, [1, self.state_size])\n",
    "        return np.argmax(self.model.predict(state, verbose=0)[0])\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "            \n",
    "        minibatch = random.sample(self.memory, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*minibatch)\n",
    "        \n",
    "        states = np.array(states)\n",
    "        actions = np.array(actions)\n",
    "        rewards = np.array(rewards)\n",
    "        next_states = np.array(next_states)\n",
    "        dones = np.array(dones)\n",
    "        \n",
    "        # Double DQN\n",
    "        current_q = self.model.predict(states, verbose=0)\n",
    "        next_q = self.model.predict(next_states, verbose=0)\n",
    "        next_target_q = self.target_model.predict(next_states, verbose=0)\n",
    "        \n",
    "        max_actions = np.argmax(next_q, axis=1)\n",
    "        targets = rewards + self.gamma * next_target_q[np.arange(self.batch_size), max_actions] * (1 - dones)\n",
    "        \n",
    "        current_q[np.arange(self.batch_size), actions] = targets\n",
    "        \n",
    "        # Обучение модели\n",
    "        history = self.model.fit(\n",
    "            states, \n",
    "            current_q, \n",
    "            batch_size=self.batch_size, \n",
    "            verbose=0\n",
    "        )\n",
    "        self.loss_history.append(history.history['loss'][0])\n",
    "        \n",
    "        # Уменьшение epsilon\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def save(self, name):\n",
    "        filename = f\"{name}.weights.h5\"\n",
    "        self.model.save_weights(filename)\n",
    "        print(f\"Модель сохранена как: {filename}\")\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(f\"{name}.weights.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d2c5fc1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arau7\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Эпизод:  1/50, Награда: -1090.20, Epsilon: 0.9950, Средняя награда: -1090.20\n",
      "Модель сохранена как: tank_dqn_best_episode_1.weights.h5\n",
      "Эпизод:  2/50, Награда: -2173.00, Epsilon: 0.9900, Средняя награда: -1631.60\n",
      "Эпизод:  3/50, Награда: -3017.00, Epsilon: 0.9851, Средняя награда: -2093.40\n",
      "Эпизод:  4/50, Награда: -114.80, Epsilon: 0.9801, Средняя награда: -1598.75\n",
      "Модель сохранена как: tank_dqn_best_episode_4.weights.h5\n",
      "Эпизод:  5/50, Награда: -427.40, Epsilon: 0.9752, Средняя награда: -1364.48\n",
      "Эпизод:  6/50, Награда: -133.20, Epsilon: 0.9704, Средняя награда: -1159.27\n",
      "Эпизод:  7/50, Награда: -1311.20, Epsilon: 0.9655, Средняя награда: -1180.97\n",
      "Эпизод:  8/50, Награда: -1025.40, Epsilon: 0.9607, Средняя награда: -1161.52\n",
      "Эпизод:  9/50, Награда: -3726.40, Epsilon: 0.9559, Средняя награда: -1446.51\n",
      "Эпизод: 10/50, Награда: -4845.80, Epsilon: 0.9511, Средняя награда: -1786.44\n",
      "Эпизод: 11/50, Награда:  -99.40, Epsilon: 0.9464, Средняя награда: -1687.36\n",
      "Модель сохранена как: tank_dqn_best_episode_11.weights.h5\n",
      "Эпизод: 12/50, Награда: -129.40, Epsilon: 0.9416, Средняя награда: -1483.00\n",
      "Эпизод: 13/50, Награда: -3464.80, Epsilon: 0.9369, Средняя награда: -1527.78\n",
      "Эпизод: 14/50, Награда: -201.60, Epsilon: 0.9322, Средняя награда: -1536.46\n",
      "Эпизод: 15/50, Награда:  -78.00, Epsilon: 0.9276, Средняя награда: -1501.52\n",
      "Модель сохранена как: tank_dqn_best_episode_15.weights.h5\n",
      "Эпизод: 16/50, Награда: -261.80, Epsilon: 0.9229, Средняя награда: -1514.38\n",
      "Эпизод: 17/50, Награда: -227.00, Epsilon: 0.9183, Средняя награда: -1405.96\n",
      "Эпизод: 18/50, Награда: -323.00, Epsilon: 0.9137, Средняя награда: -1335.72\n",
      "Эпизод: 19/50, Награда: -591.60, Epsilon: 0.9092, Средняя награда: -1022.24\n",
      "Эпизод: 20/50, Награда: -1185.80, Epsilon: 0.9046, Средняя награда: -656.24\n",
      "Эпизод: 21/50, Награда: -152.40, Epsilon: 0.9001, Средняя награда: -661.54\n",
      "Эпизод: 22/50, Награда: -1236.40, Epsilon: 0.8956, Средняя награда: -772.24\n",
      "Эпизод: 23/50, Награда: -359.00, Epsilon: 0.8911, Средняя награда: -461.66\n",
      "Эпизод: 24/50, Награда: -1272.80, Epsilon: 0.8867, Средняя награда: -568.78\n",
      "Эпизод: 25/50, Награда: -124.40, Epsilon: 0.8822, Средняя награда: -573.42\n",
      "Эпизод: 26/50, Награда: -533.80, Epsilon: 0.8778, Средняя награда: -600.62\n",
      "Эпизод: 27/50, Награда: -120.80, Epsilon: 0.8734, Средняя награда: -590.00\n",
      "Эпизод: 28/50, Награда: -857.80, Epsilon: 0.8691, Средняя награда: -643.48\n",
      "Эпизод: 29/50, Награда:   72.40, Epsilon: 0.8647, Средняя награда: -577.08\n",
      "Модель сохранена как: tank_dqn_best_episode_29.weights.h5\n",
      "Эпизод: 30/50, Награда: -280.40, Epsilon: 0.8604, Средняя награда: -486.54\n",
      "Эпизод: 31/50, Награда: -246.00, Epsilon: 0.8561, Средняя награда: -495.90\n",
      "Эпизод: 32/50, Награда: -260.20, Epsilon: 0.8518, Средняя награда: -398.28\n",
      "Эпизод: 33/50, Награда: -547.00, Epsilon: 0.8475, Средняя награда: -417.08\n",
      "Эпизод: 34/50, Награда:   79.20, Epsilon: 0.8433, Средняя награда: -281.88\n",
      "Модель сохранена как: tank_dqn_best_episode_34.weights.h5\n",
      "Эпизод: 35/50, Награда: -185.60, Epsilon: 0.8391, Средняя награда: -288.00\n",
      "Эпизод: 36/50, Награда:   35.80, Epsilon: 0.8349, Средняя награда: -231.04\n",
      "Эпизод: 37/50, Награда: -160.40, Epsilon: 0.8307, Средняя награда: -235.00\n",
      "Эпизод: 38/50, Награда: -220.40, Epsilon: 0.8266, Средняя награда: -171.26\n",
      "Эпизод: 39/50, Награда: -217.20, Epsilon: 0.8224, Средняя награда: -200.22\n",
      "Эпизод: 40/50, Награда: -324.00, Epsilon: 0.8183, Средняя награда: -204.58\n",
      "Эпизод: 41/50, Награда: -121.80, Epsilon: 0.8142, Средняя награда: -192.16\n",
      "Эпизод: 42/50, Награда: -112.40, Epsilon: 0.8102, Средняя награда: -177.38\n",
      "Эпизод: 43/50, Награда: -145.80, Epsilon: 0.8061, Средняя награда: -137.26\n",
      "Эпизод: 44/50, Награда:  -97.00, Epsilon: 0.8021, Средняя награда: -154.88\n",
      "Эпизод: 45/50, Награда: -251.80, Epsilon: 0.7981, Средняя награда: -161.50\n",
      "Эпизод: 46/50, Награда: -525.00, Epsilon: 0.7941, Средняя награда: -217.58\n",
      "Эпизод: 47/50, Награда:   80.00, Epsilon: 0.7901, Средняя награда: -193.54\n",
      "Модель сохранена как: tank_dqn_best_episode_47.weights.h5\n",
      "Эпизод: 48/50, Награда:  -92.60, Epsilon: 0.7862, Средняя награда: -180.76\n",
      "Эпизод: 49/50, Награда: -186.60, Epsilon: 0.7822, Средняя награда: -177.70\n",
      "Эпизод: 50/50, Награда:  -94.00, Epsilon: 0.7783, Средняя награда: -154.70\n",
      "Модель сохранена как: tank_dqn_final.weights.h5\n",
      "\n",
      "Обучение завершено!\n",
      "\n",
      "Результаты обучения за 50 эпизодов:\n",
      "Максимальная награда: 80.00\n",
      "Минимальная награда: -4845.80\n",
      "Средняя награда: -657.70\n",
      "Средняя награда (последние 10): -154.70\n",
      "\n",
      "График истории наград сохранен в rewards_history.png\n"
     ]
    }
   ],
   "source": [
    "# Параметры обучения\n",
    "EPISODES = 50\n",
    "TARGET_UPDATE_FREQ = 5\n",
    "\n",
    "# Инициализация среды и агента\n",
    "env = TankEnv()\n",
    "state_size = 14\n",
    "action_size = 5\n",
    "\n",
    "agent = DQNAgent(state_size, action_size)\n",
    "episode_rewards = []\n",
    "best_score = -float('inf')\n",
    "\n",
    "# Основной цикл обучения\n",
    "running = True\n",
    "for episode in range(EPISODES):\n",
    "    if not running:\n",
    "        break\n",
    "        \n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    \n",
    "    # Первоначальная отрисовка\n",
    "    if not TRAINING_MODE:\n",
    "        running = env.render(\n",
    "            episode=episode+1, \n",
    "            total_reward=total_reward, \n",
    "            action=None,\n",
    "            speed=RENDER_SPEED\n",
    "        )\n",
    "    \n",
    "    while not done and running:\n",
    "        # Выбор действия\n",
    "        action = agent.act(state)\n",
    "        \n",
    "        # Выполнение действия\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        # Сохранение опыта\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        \n",
    "        # Обновление состояния\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        \n",
    "        # Визуализация\n",
    "        if not TRAINING_MODE:\n",
    "            running = env.render(\n",
    "                episode=episode+1, \n",
    "                total_reward=total_reward, \n",
    "                action=action,\n",
    "                speed=RENDER_SPEED\n",
    "            )\n",
    "    \n",
    "    # Сохранение статистики\n",
    "    episode_rewards.append(total_reward)\n",
    "    \n",
    "    # Обучение на опыте\n",
    "    agent.replay()\n",
    "    \n",
    "    # Периодическое обновление целевой сети\n",
    "    if episode % TARGET_UPDATE_FREQ == 0:\n",
    "        agent.update_target_model()\n",
    "    \n",
    "    # Вывод статистики\n",
    "    print(f\"Эпизод: {episode+1:2d}/{EPISODES}, \"\n",
    "          f\"Награда: {total_reward:7.2f}, \"\n",
    "          f\"Epsilon: {agent.epsilon:.4f}, \"\n",
    "          f\"Средняя награда: {np.mean(episode_rewards[-10:] if episode_rewards else 0):.2f}\")\n",
    "    \n",
    "    # Сохранение лучшей модели\n",
    "    if total_reward > best_score:\n",
    "        best_score = total_reward\n",
    "        agent.save(f\"tank_dqn_best_episode_{episode+1}\")\n",
    "\n",
    "# Сохранение финальной модели\n",
    "if running:\n",
    "    agent.save(\"tank_dqn_final\")\n",
    "    print(\"\\nОбучение завершено!\")\n",
    "\n",
    "    # Анализ результатов\n",
    "    if episode_rewards:\n",
    "        print(f\"\\nРезультаты обучения за {min(episode+1, EPISODES)} эпизодов:\")\n",
    "        print(f\"Максимальная награда: {max(episode_rewards):.2f}\")\n",
    "        print(f\"Минимальная награда: {min(episode_rewards):.2f}\")\n",
    "        print(f\"Средняя награда: {np.mean(episode_rewards):.2f}\")\n",
    "        if len(episode_rewards) >= 10:\n",
    "            print(f\"Средняя награда (последние 10): {np.mean(episode_rewards[-10:]):.2f}\")\n",
    "\n",
    "        # Сохранение истории наград\n",
    "        with open(\"rewards_history.txt\", \"w\") as f:\n",
    "            for i, reward in enumerate(episode_rewards):\n",
    "                f.write(f\"{i+1},{reward}\\n\")\n",
    "\n",
    "        # График наград\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(episode_rewards, label='Награда за эпизод')\n",
    "        \n",
    "        if len(episode_rewards) >= 10:\n",
    "            moving_avg = [np.mean(episode_rewards[max(0, i-9):i+1]) for i in range(len(episode_rewards))]\n",
    "            plt.plot(moving_avg, 'r-', label='Скользящее среднее (10 эп.)')\n",
    "        \n",
    "        plt.title(\"История наград во время обучения\")\n",
    "        plt.xlabel(\"Эпизод\")\n",
    "        plt.ylabel(\"Награда\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"rewards_history.png\")\n",
    "        plt.close()\n",
    "\n",
    "        print(\"\\nГрафик истории наград сохранен в rewards_history.png\")\n",
    "\n",
    "pygame.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
